{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align= \"center\">\n",
    "\n",
    "<h1>Memoria</h1>\n",
    "<h2>Estimación de precios de casas</h2>\n",
    "<h2>Machine Learning - The Bridge 2024 - Data Science </h2>\n",
    "\n",
    "![migracion_img.png](./src/img/The_bridge_logo.png)\n",
    "\n",
    "**<h2> Viridiana Espinosa </h2>**\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Temario](#1)\n",
    "-----\n",
    "1. ###### [Introducción](#introduccion)\n",
    "2. ###### [Objetivo](#objetivo)\n",
    "3. ###### [Tipo de problema](#problema)\n",
    "4. ###### [Procedimiento y tratamiento de datos](#procedimiento)\n",
    "    * ###### [Fuentes](#fuentes)\n",
    "    * ###### [Librerías](#librerias)\n",
    "    * ###### [Tratamiento de datos](#tratamiento)\n",
    "        * ###### [Extracción y lectura de datos](#extraccion)\n",
    "    * ###### [Mini EDA](#eda)\n",
    "        * ###### [Revisión y análisis de Target y Variables](#target)\n",
    "            * ###### [Correlación de Pearson](pearson)\n",
    "            * ###### [Comprobación de Colinealidad](#colinealidad)\n",
    "            * ###### [Variables categóricas y su influencia en la predicción](#categoricas)\n",
    "            * ###### [Análisis de variables númericas y transformación](#numericas)\n",
    "5. ###### [Aplicación de los pasos del train al test](#aplicacion)\n",
    "6. ###### [Modelado](#modelado)\n",
    "6. ###### [Conclusiones](#conclu)\n",
    "\n",
    "----- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción <a id=\"introduccion\"></a>\n",
    "\n",
    "Una agencia inmobiliaria desea mejorar sus estrategias de venta y toma de decisiones al estimar los precios de las viviendas en una ciudad específica. Esta agencia cuenta con un conjunto de datos históricos que incluyen características de las propiedades (como número de habitaciones, baños, ubicación, antigüedad, entre otros) y sus precios de venta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivo <a id=\"objetivo\"></a>\n",
    "\n",
    "El objetivo es construir un modelo de predicción que, a partir de las características de una casa, pueda estimar disminuir el márgen de error que actualmente tiene en su media de 537k su precio en el mercado. Este modelo ayudará a la agencia a ofrecer estimaciones rápidas a sus clientes y a definir precios más competitivos basados en las características de cada propiedad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tipo de problema: <a id=\"problema\"></a>\n",
    "\n",
    "Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedimiento y tratamiento de datos <a id=\"procedimiento\"></a>\n",
    "\n",
    "### Fuentes <a id=\"fuentes\"></a>\n",
    "\n",
    "* Se utilizaron datos de Kaggle [Kaggle](https://www.kaggle.com/datasets/sukhmandeepsinghbrar/housing-price-dataset?select=Housing.csv)\n",
    "\n",
    "# Librerías <a id=\"librerias\"></a>\n",
    "\n",
    "* import pandas as pd\n",
    "* import seaborn as sns\n",
    "* from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "* from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "* import matplotlib.pyplot as plt\n",
    "* import numpy as np\n",
    "* from scipy import stats\n",
    "* from sklearn.metrics import root_mean_squared_error\n",
    "* from sklearn.preprocessing import StandardScaler\n",
    "* from sklearn.preprocessing import OrdinalEncoder\n",
    "* from sklearn.linear_model import LinearRegression\n",
    "* from sklearn.ensemble import RandomForestRegressor\n",
    "* from xgboost import XGBRegressor\n",
    "* from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "* from sklearn.tree import DecisionTreeRegressor\n",
    "* from lightgbm import LGBMRegressor\n",
    "* from catboost import CatBoostRegressor\n",
    "\n",
    "# Tratamiento de datos <a id=\"tratamiento\"></a>\n",
    "\n",
    "### Lectura de datos, exporación y división de Train y Test <a id=\"extraccion\"></a>\n",
    "\n",
    "1. En esta parte se ha procedido a crear el código para realizar la extracción de los datos, donde también se ha designado el Target (precio). \n",
    "2. Adicionalmente en esta sección se hace la división de Train y Test que nos servirá más adelante para cuando hagamos el entrenamiento de los módelos. Cabe resaltar que inicialmente se había separado directamente en X e Y de train y test, sin embargo, cuando se siguió avanzando con el proyecto se determinó que era más eficiente iniciar con una división de train y test y después dividir por X e Y.\n",
    "3. Una vez efectuada la división de train y test se comienza con la exploración de los datos, donde observamos:\n",
    "    * Que tenemos 21 columnas en total incluyendo el target \n",
    "    * Se observa de primer instancia que no hay nulos que necesiten tratamiento \n",
    "    * Se observa que de 21 variables solo una es de tipo objeto (date) y el resto son númericas\n",
    "4. Se utiliza una función para poder verificar el porcentaje de nulos y de cardinalidad donde podemos observar que la variable \"id\" tiene una cardinalidad muy alta (probablemente la descartaremos) y que el porcentaje de  nulos es cero.\n",
    "\n",
    "\n",
    "## Mini EDA <a id=\"eda\"></a>\n",
    "\n",
    "### Revisión y análisis de Target y Variables <a id=\"target\"></a>\n",
    "    \n",
    "* Se obseva que el target tiene una distribución normal.\n",
    "* Se observa que hay varias variables que necesitarán tratamiento debido a outliers, aunque no se comenzará con ningún tratamiento hasta determinar antes que variables son las que se utilizarán en el nódelo\n",
    "* Se utiliza una función para determinar y podernos ayudar a verificar el tipo de variables, aunque conforme vamos avanzando con el análisis de las variables nos vamos percatando que efectivamente hay variables núemricas discretas que son en realidad categóricas ordinales, esto es importante ya que el tratamiento a utilizar en las variables es diferente que las númericas (por ejemplo ()\"grade\").\n",
    "* Se observa que que según la desviación estandár (std) nos dice que hay algunas de las variables con outliers potentes (ya vistos anteriormete en las gráficas), por ejemplo, sqt_living, sqft_lot, grade, sqtf_above, sqft_basement, yr_built, yr_renovated, zipcode, sqft_living15 ysqft_lot15*\n",
    "\n",
    "Nota: Los datos análizados son con base al train únicamente.\n",
    "\n",
    "#### Correlación de Pearson <a id=\"pearson\"></a>\n",
    "\n",
    "*Se aplica la correlación de pearson al train*\n",
    "\n",
    "* Se obtiene la tabla de la correlación de pearson, esta tabla nos ayuda a saber la correlación con el target para poder ayudarnos a determinar si hay variables que podamos quitar.\n",
    "* Se decide que las variables por debajo de 0.09 no se utilizaran como variables de entrenamiento en el módelo. \n",
    "\n",
    "##### División de features númericas de features categóricas\n",
    "\n",
    "* Basado en el criterio de 0.09 se eliminan todas las variables que no cumplen este criterio y se alocan cono features númericas.\n",
    "* Una vez que tenemos todas las variables que cumplen con el criterio dentro de features númericas pasamos a remover las variables que hemos identificado como catégoricas \n",
    "    * Estás variables categóricas salieron de la función que nos ayudo a identificar el tipo de variable, pero también del análisis que se hizo a posterior análizando (basado en los comentarios que se tenían del propósito de cada variable) que efectivamente su tipo de variable fuera efectivamente númerica o en su defecto categórica ordinal.\n",
    "    * Este análisis se llevo acabo obteniendo los valores únicos de estás variables y así pudiendo confimar que el mejor tratamiento de estas variables sería categóricas: bathrooms, bedrooms, yr_renovated, floors.\n",
    "\n",
    "Quedando de la siguiente manera:\n",
    "\n",
    "* Features númericas: sqft_living, sqft_above, sqft_living15, view, sqft_basement, lat, sqft_lot. \n",
    "* Features categóricas: floors, condition, grade, waterfront, yr_renovated, bathrooms, bedrooms\n",
    "\n",
    "#### Comprobación de colinealidad <a id=\"colinealidad\"></a>\n",
    "\n",
    "* Se aplica un for para excluir aquellas variables que tienen una colinealidad superior a 0.7 (este número es el típicamente utilizado por lo cual hemos mantenido el mismo como referencia)\n",
    "* Basado en esto podemos observar que en la lista de \"excluidas\" se tienen sqft_above y sqft_living15 y que se mantienen como **features númericas sqft_living, view, sqft_basement, lat, sqft_lot.**\n",
    "* Realizamos un check más que es mirar la correlación con el target de estas variables para corroborar que efectivamente estas variables son las que ya sea se deben mantener como valores númericas o ser excluídas. \n",
    "\n",
    "#### Determinación de variables categóricas y su influencia en la predicción <a id=\"categoricas\"></a>\n",
    "\n",
    "* Se aplican diagramas de barras a las variables categóricas para observar su relación con el target y por lo tanto si son candidatas a ser consideradas como variables de entrenamiento del módelo.\n",
    "* Se determina que se usarán las 7 catégoricas ya que vemos cortes.\n",
    "* El criterio que se decide utilizar para el tratamiento de las variables es **Ordinal Encoding**\n",
    "\n",
    "***Ordinal Encoding***\n",
    "\n",
    "* Comenzamos a categorizar las variables según el número de categorías, por ejemplo, si había 1,2,3,4,5 cuartos, entonces asignábamos A.B,C,D,E,F. Sin embargo, cuando fuimos avanzando con el modelado, caímos en ceunta que esto nos generaba en algunos casos categorías con pocas representaciones de clases, es decir, si por ejemplo 15 cuartos podría a llegar a tener solo 20 datos representativos de 15 mil y esto afectaría al modelo en sus predicciones al tener tantos outliers, por lo cuál se ha tenido que hacer primero una agrupación por valores , por ejemplo 5 en 5, para poder pasar después a clasificarlas en A,B,C, esto nos ayudo a incrementar la población representada en una proporción más adecuada e incluso poder disminuir el número de clasificaciones. \n",
    "* Una vez asignadas las clasificaciones a cada una de las variables, se procedió a afectuar el ordinal encoding. \n",
    "* El siguiente paso fue comprobar la distribución de los datos después de la aplicación del ordinal encoding, para comprobar que efectivamente la distribución de la representación de la muestra en cada variable fuera adecuada. Se observo que waterfront y yr_renovated se tendrían que excluir puesto que la proporción era bastante desproporcionada, por ejemplo, waterfront en 0 tenía 17k datos, mientras que en 1 tenía 124 datos.\n",
    "\n",
    "Por lo que finalemente los features categóricas que quedaron seleccionadas y que serían utilizadas en el modelo son: ordinal_floors, ordinal_bedrooms, ordinal_condition, ordinal_grade y ordinal_bathrooms.\n",
    "\n",
    "#### Análisis de variables númericas y transformación <a id=\"numericas\"></a>\n",
    "\n",
    "* Un dato importante a mencionar es que \"view\" originalmente se había considerado como categórica, sin embargo, después de hacer la clasificación del ordinal encoding y la proporción, se considero mejor mantenerla como una variable númerica.\n",
    "* Se efectúa la visualización con histogramas para ver que variables necesitan tratamiento, se observa que a las 5 variables le aplicaremos logartimo + 1 ya que tenemos ceros y al modelo no le sienta bien estos y procederemos con el escalado, esto lo hacemos para mejorar la distribución de los datos. \n",
    "* Realizamos un nuevo train pero con los datos escalados, de esta forma ahora tenemos el train sin escalar y el train escalado.\n",
    "\n",
    "# Aplicación de los pasos del train al test <a id=\"aplicacion\"></a>\n",
    "\n",
    "* Se han aplicado y replicado todos los pasos efectuados del train al test.\n",
    "\n",
    "##### División X,y\n",
    "\n",
    "* En este apartado hemos procedido a hacer la división de X e Y.\n",
    "* Para esto se han unido tanto las variables categóricas como las variables númericas en una variable llamada \"features_all\".\n",
    "* Se ha procedido a aplicar features_all como parte del data set de X_train y de X_test \n",
    "* Se procede a colocar al target en Y_train e Y_test \n",
    "* Se valida con un info() que no haya datos NaN\n",
    "\n",
    "Nota: Lo mismo se aplico en los data ser escalados.\n",
    "\n",
    "##### Extracción de datos procesados\n",
    "\n",
    "* Se procede a hacer la extracción de los datos ya procesados y que son los que se utilizarán en el módelo\n",
    "\n",
    "# Modelado <a id=\"Modelado\"></a>\n",
    "\n",
    "* Se verifica cual es la media del dataset de train, ya que el objetivo es dismimuir en un 30% este error en la estimación de precios.\n",
    "* Se evaluan los siguientes modelos: \"Regresion Lineal\",\"DecisionTree\",\"Random Forest\",\"XGBoost\",\"LightGBM\",\"CatBoost\"\n",
    "* Se aplica la comparación con validación cruzada.\n",
    "    * En un principio se había utilizado el set escalado solo para la regresión lineal, pero al final, se decidió que se aplicaría para todos los modelos. Esto porque investigando un poco más el escalado no afecta a los otros modelos. Se hizo una comparación de las métricas aplicando escalado y no escalado y efectivamente, no variaban mucho.\n",
    "* Se observo que el peor modelo era la regresión lineal y que los dos mejores modelos eran el CatBoost y el Random Forest \n",
    "\n",
    "#### Determinación del modelo ganador\n",
    "\n",
    "Se determina que el modelo ganador es el RandomForest ya que cuando se quizo hacer la optimización de parámetros de CatBoost, tomo toda una noche obtener el resultado y aún así no obtuvimos un número, y la variación entre CatBoost y RandomForest no era mucha.\n",
    "\n",
    "#### Optimización del los hiperparámetros del modelo ganador (RandomForest)\n",
    "\n",
    "* Aquí tuvimos un poco de dificultad para poder ir disminuyendo el margen de error, puesto que si se metían varios parámetros en GridSearch, entonces, el tiempo de procesamiento se propolongaba, por lo que se hicieron varias pruebas con diferentes segmentos, como: diferente cantidad de árboles (estimadores), diferentes profundidades, diferentes número de features que uilizaba cada split (max_depth) y como *métrica* se utilizó el RSME.\n",
    "\n",
    "#### Aplicación del modelo ganador en X_test \n",
    "\n",
    "Se aplica el modelo ganador a X_test e Y_test utilizando predict \n",
    "\n",
    "##### Resultado estimación de precio\n",
    "\n",
    "* Random Forest sin optimización 163k\n",
    "* Random Forest con optimización 161k\n",
    "* Random Forest aplicado a test 184k\n",
    "\n",
    "Disminución de error vs 537k igual a un 30%.\n",
    "\n",
    "#### Resultado Métricas aplicadas a test\n",
    "\n",
    "* MAE: 48841.83885714284\n",
    "* RMSE: 184256.07283383913\n",
    "* Precio medio 537768.5916136495\n",
    "\n",
    "Nota: Se observo que por los outliers, el modelo calcularía o estimaría mejor los precios de las casas por debajo de un millón, esto debido a los outliers.\n",
    "\n",
    "#### Otras métricas optimizadas\n",
    "\n",
    "* Ya que no se pudo extraer ocn GridSearch los resultados de CatBoost, se probó con RandomSearch y se obtuvo un resultado de 161k por lo cual, podemos ver que efectivamente, el usar el modelo RandomForest es el mejor.\n",
    "\n",
    "# Conclusiones <a id=\"conclu\"></a>\n",
    "\n",
    "* Algunas variables númericas discretas tuvieron que convertirse a categóricas ordinales como baños, cuartos, etc..\n",
    "* Se excluyeron aquellas variables que tenían poca relación con el target o mucha relación entre sí (seleccionando solo una)\n",
    "* Se exluyeron aquellas variables que en proporción con su categoría se excedía más del 80/20 - ejemplo waterfront sin (17k) y con (124k)\n",
    "* A mayor cantidad de condiciones, mayor es el outlier - ejemplo, entre más cuartos menor representación de población\n",
    "* El mejor modelo fue el CatBoost, sin embargo, el tiempo de optimización es más costoso en tiempo y herramientas, por lo cual se ha seleccionado a RandomForest siendo el segundo mejor en métricas y el mejor en relación coste/eficiencia\n",
    "\n",
    "* Random Forest Regressor\n",
    "    * Obtención de resultado más eficiente\n",
    "    * Mejora de desviación en estimación de precios vs mediana de un 30% ($161k/$537k)\n",
    "\n",
    "* Observamos que incluso optimizando los hipérparametros en CatBoost, la desviación/error sigue en un 30%, por lo que esto nos confirma quedarnos con Random Forest Regressor \n",
    "* No se elige GridSearch como contraste puesto que:\n",
    "    * El tiempo de obtención de resultados, sigue siendo más rápido en RF. Adicionalmente, la variación sigue estando en un 30%. Puesto que en caso de querer seguir haciendo mejoras a través de los hiperparámetros RF se mantiene como la mejor opción.\n",
    "\n",
    "A nivel negocio podemos concluir que este modelo les ayudará a disminuir el márgen de error que tenían de media en un 30%, por lo que les será útil para mejor sus precios y competencia en el mercado inmobiliario.*\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
